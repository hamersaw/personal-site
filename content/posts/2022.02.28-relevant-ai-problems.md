---
title: "Relevant AI Problems"
date: 2022-02-28T09:56:10-06:00
draft: false
author: "Daniel Rammer"

tags: ["100DaysToOffload", "Opinion", "Research"]
---

There is a popular opinion article [Too many AI researchers think real-world problems are not relevant](https://www.technologyreview.com/2020/08/18/1007196/ai-research-machine-learning-applications-problems-opinion/) that has come across my LinkedIn feed on multiple occasions. I can only, in good concious, scan past it 2 or 3 times before I feel responsible to understand why this issue is so important that it keeps repeating.

The subtitle, namely "The communityâ€™s hyperfocus on novel methods ignores what's really important", provides more insight into the argument. In synopsis, research on domain-specific applications of AI techniques is too frequently rejected from top-level conferences and publications. This prompts question into the greater direction of AI research and the disconnect between research and solving real-world problems.

This is an interesting piece, and the popularity speaks to the importance. It resonates with me particularly given my recent transition from academia to industry. The foundational aspects remain a prominent issue in publishing. At a high-level I think the research community fails to address deep-rooted shortcomings in incentivization, most notable the reward of quantity publishing rather than innovative, quality work.

My research experience is in distributed systems, with portions flirting on the boundaries of AI. Specifically, our attempts to address inefficiencies in scaling modern ML / AI pipelines for spatiotemporal data. As the current "hot topic" in Computer Science, AI is seeing unprecedented attention. Consequently, very little of this work is relatively innovative, but small steps are often required to produce large results. In our case, the inclusion of an AI application widened the scope of our work and in reaching more audiences allowed us to published quite well.

However, in all of our work explaining the viability of our approach across domains was paramount, where our implementations and benchmarks were arguably translatable. This allows us to repeatedly position the work as a conceptual improvement in the realm of distributed data storage and analytics rather than specific to the spatiotemporal data processing domain. Therefore, the work fit well into publications on systems-level improvements scaling data solutions rather than a domain-specific publication. Although feedback into the domain-specific nature of our work was often warranted, received, and addressed. 

To circle back to the original article, at risk of exposing my naivety, my understanding is that the topic work takes existing AI techniques and applies them to domain-specific problems, tweaking parameters to improve solutions. While the results may be promising, it hardly seems to be a conceptual innovation in AI. Rather, the work may better relate to a conference within the applied domain whose goals include furthering solutions thereof rather than conceptual AI innovation.

It is difficult to define the divide between which work addresses conceptual innovation rather than domain-specific applications. And even more difficult to accept / reject certain works based on those grounds. I think, with an obviously un-biased opinion based on my previous work, that research into domain-specific applications requires some level of inter-domain support to be viable for a Computer Science conference, otherwise it may be more applicable to a publication within that domain. Similarly, conceptual innovations require a strong attachment to real-world problems.

_8 day(s) offloaded in the [100DaysToOffload](https://100daystooffload.com/) challenge._
